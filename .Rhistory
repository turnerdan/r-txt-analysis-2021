gutenberg_sherlock[detected] # 2 instances, 1 in table of contents, 2 in the text
# Use str_locate_all to locate every match we detected
located <- as.data.frame(cbind(1:length(detected),
detected,
str_locate_all(gutenberg_sherlock,
target_title)))
colnames(located) <- c("line", "detected", "location")
# Get the start of the span (end of the title)
start <- located %>% filter(detected == TRUE) %>%
select(line) %>% # select the line number col
last() %>% # to access the line numbers inside the col
nth(2) # second instance
# Get the end of the span (following title)
detected <- str_detect(gutenberg_sherlock, following_title)
# Check which match we will designate as our end point
gutenberg_sherlock[detected] # 2 instances, 1 in table of contents, 2 in the text, 3 is something else
located <- as.data.frame(cbind(1:length(detected),
detected,
str_locate_all(gutenberg_sherlock, following_title)))
colnames(located) <- c("line", "detected", "location")
end <- located %>% filter(detected == TRUE) %>%
select(line) %>% # select the line number col
last() %>% # to access the line numbers inside the col
nth(2) # second instance
# Extraction
three_quarter <- gutenberg_sherlock[start + 1 : end - 1]
head(three_quarter) # Peek
wiki_url <- "https://en.wikipedia.org/wiki/List_of_Chicago_%22L%22_stations"
wiki_html <- read_html(wiki_url)
wiki_table <- wiki_html %>%
html_node("#mw-content-text > div.mw-parser-output > table:nth-child(13)") %>%
html_table()
View(wiki_table)
wiki_url <- "https://en.wikipedia.org/wiki/Timeline_of_geopolitical_changes_(1900%E2%88%92present)"
wiki_html <- read_html(wiki_url)
wiki_table <- wiki_html %>%
html_node("#mw-content-text > div.mw-parser-output > table:nth-child(9)") %>%
html_table()
View(wiki_table)
wiki_table <- wiki_html %>%
html_node("#mw-content-text > div.mw-parser-output > table") %>%
html_table()
wiki_table <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table") %>%
html_table()
View(wiki_table)
wiki_table <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table")
wiki_tables <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table")
View(wiki_tables)
wiki_table <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table") %>%
html_table()
wiki_tables <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table") %>%
html_table()
wiki_tables[1]
head(wiki_tables[2])
head(wiki_tables[3])
wiki_tables[[1]]
wiki_tables[2
]
x<- rbind(wiki_tables[2:14])
View(x)
x<- rbind(wiki_tables[2:14])
typeof(wiki_tables[1])
wiki_tables[1]
wiki_tables[1][[1]]
wiki_tables[1]
wiki_tables[1]
wiki_tables[[1]]
head(wiki_tables)
wiki_tables[[1]]
head(wiki_tables[[1]])
head(wiki_tables[[1]])
wiki_tables
wiki_tables[[1]]
wiki_tables[[1]][1]
wiki_tables[[14]]
wiki_tables[[2]]
x<- rbind(wiki_tables[[2:14]])
x<- wiki_tables[[2:14]]
bind_rows(wiki_tables, .id = "column_label")
x <- bind_rows(wiki_tables[[2:14]], .id = "column_label")
View(x)
View(x[[1]][[1]])
View(x[[2]][[1]])
View(x[[5]][[1]])
x <- bind_rows(wiki_tables, .id = "column_label")
View(x)
x <- bind_rows(wiki_tables)
View(x)
?bind_rows
View(wiki_tables)
View(x)
x <- bind_rows(wiki_tables, .id = "column_label")
wiki_tables <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table") %>%
html_table() %>%
bind_rows(.id = "tbl order")
View(wiki_tables)
wiki_tables <- wiki_html %>%
html_nodes("#mw-content-text > div.mw-parser-output > table") %>%
html_table() %>%
bind_rows(.id = "tbl_order")
wiki_tables %>% sample_n(10)
# Subset
wiki_tables <- wiki_tables %>% select(c("Year", "Event"))
View(wiki_tables)
# Subset
wiki_tables <- wiki_tables %>%
select(c("Year", "Event")) %>% # Keep only these columns
filter(na.omit())
# Subset
wiki_tables <- wiki_tables %>%
select(c("Year", "Event")) %>% # Keep only these columns
drop_na() %>%
first()
# Subset
wiki_tables <- wiki_tables %>%
select(c("Year", "Event")) %>% # Keep only these columns
na.omit()
# install.packages("stringi")
# install.packages("stringr")
# install.packages("tokenizers")
# install.packages("gutenbergr")
# install.packages("tidytext")
# install.packages("rvest")
# Load packages
library(dplyr) # modern syntax
# library(tokenizers) # consistent, convenient text tokenization
library(gutenbergr) # easy access to gutenberg digital library
library(stringr) # most common string processing functions (uses stringi)
library(tidytext) # most common text processing functions
library(rvest)
# Find the gutenberg_id of a book to scrape
# Return only books where 'Doyle' is detected in the 'author' position
gutenberg_ids <- gutenberg_works(str_detect(author, "Doyle"))
# For example
gutenberg_sherlock <- gutenberg_download(108)$text # run this and check the result
# Download the raw text of your choice
# gutenberg_text <- gutenberg_download(108)$text
# A non-generalized way of doing the same thing
# gutenberg_ids <- gutenberg_works(author == "Doyle, Arthur Conan")
# Here are some ids of popular books:
# 1342 = Pride and Prejudice by Jane Austen
# 11 = Alice's Adventures in Wonderland by Lewis Carroll
# 64317 = The Great Gatsby by F. Scott Fitzgerald
# For The Return of Sherlock Holmes, the text begins on line 33
head(gutenberg_download(108)$text, 40)
# Old fashioned way
# The lines we want
gutenberg_sherlock[16:28] # note leading whitespace
# Import adventure list as a clean tibble
sherlock_adventures <- gutenberg_sherlock[16:28] %>%
trimws() %>% # trim extra whitespace around each line
str_replace_all("[\\p{P}\\p{S}&&[^-]]", "")# replace all punctuation (p) with a space (second argument) except (Substract) hyphens
# Compare
gutenberg_sherlock[21] # raw
sherlock_adventures[6] # clean
# We could also find these using a regular expression (regex)
# Convert the text to lowercase
sherlock_adventures <- tolower(sherlock_adventures)
gutenberg_sherlock <- tolower(gutenberg_sherlock)
# Find the part of the file to extract (end of target title to beginning of following title)
target_title <- sherlock_adventures[11] # The Adventure of the Missing Three-Quarter is the 11th element in the table of contents we scraped
following_title <- sherlock_adventures[12] # The title of the next story will help us know where to stop
# Now let's find the boundaries of the text span we want to extract
# Detect the target title
hits <- str_detect(gutenberg_sherlock, target_title)
# Which "hit" do we want to extract?
gutenberg_sherlock[detected] # 2 instances, 1 in table of contents, 2 in the text
# Convert the text to lowercase
sherlock_adventures <- tolower(sherlock_adventures)
gutenberg_sherlock <- tolower(gutenberg_sherlock)
# Find the part of the file to extract (end of target title to beginning of following title)
target_title <- sherlock_adventures[11] # The Adventure of the Missing Three-Quarter is the 11th element in the table of contents we scraped
following_title <- sherlock_adventures[12] # The title of the next story will help us know where to stop
# Now let's find the boundaries of the text span we want to extract
# Detect the target title
hits <- str_detect(gutenberg_sherlock, target_title)
# Which "hit" do we want to extract?
gutenberg_sherlock[hits] # 2 instances, 1 in table of contents, 2 in the text
# Use str_locate_all to locate every match we detected
hits <- as.data.frame(cbind(1:length(hits),
hits,
str_locate_all(gutenberg_sherlock,
target_title)))
colnames(hits) <- c("line", "detected", "location")
# Get the start of the span (end of the title)
start <- hits %>% filter(detected == TRUE) %>%
select(line) %>% # select the line number col
last() %>% # to access the line numbers inside the col
nth(2) # second element from gutenberg_sherlock[detected]
# Get the end of the span (following title)
hits <- str_detect(gutenberg_sherlock, following_title)
# Which "hit" do we want to extract?
gutenberg_sherlock[hits] # 3 instances, 1 in table of contents, 2 in the text, 3 is something else
hits <- as.data.frame(cbind(1:length(hits),
hits,
str_locate_all(gutenberg_sherlock,
following_title)))
colnames(hits) <- c("line", "detected", "location")
end <- hits %>% filter(detected == TRUE) %>%
select(line) %>% # select the line number col
last() %>% # to access the line numbers inside the col
nth(2) # second element from gutenberg_sherlock[detected]
# Extraction
sherlock_three_quarter <- gutenberg_sherlock[start + 1 : end - 1]
head(sherlock_three_quarter) # Peek
# What additional processing do you think we need?
# Check result
wiki_tables %>% sample_n(10)
# install.packages("stringi")
# install.packages("stringr")
# install.packages("tokenizers")
# install.packages("gutenbergr")
# install.packages("tidytext")
# install.packages("rvest")
# Load packages
library(dplyr) # modern syntax
# library(tokenizers) # consistent, convenient text tokenization
library(gutenbergr) # easy access to gutenberg digital library
library(stringr) # most common string processing functions (uses stringi)
library(tidytext) # most common text processing functions
library(rvest)
# Find the gutenberg_id of a book to scrape
# Return only books where 'Doyle' is detected in the 'author' position
gutenberg_ids <- gutenberg_works(str_detect(author, "Doyle"))
# For example
gutenberg_sherlock <- gutenberg_download(108)$text # run this and check the result
# Download the raw text of your choice
# gutenberg_text <- gutenberg_download(108)$text
# A non-generalized way of doing the same thing
# gutenberg_ids <- gutenberg_works(author == "Doyle, Arthur Conan")
# Here are some ids of popular books:
# 1342 = Pride and Prejudice by Jane Austen
# 11 = Alice's Adventures in Wonderland by Lewis Carroll
# 64317 = The Great Gatsby by F. Scott Fitzgerald
# For The Return of Sherlock Holmes, the text begins on line 33
head(gutenberg_download(108)$text, 40)
# Old fashioned way
# The lines we want
gutenberg_sherlock[16:28] # note leading whitespace
# Import adventure list as a clean tibble
sherlock_adventures <- gutenberg_sherlock[16:28] %>%
trimws() %>% # trim extra whitespace around each line
str_replace_all("[\\p{P}\\p{S}&&[^-]]", "")# replace all punctuation (p) with a space (second argument) except (Substract) hyphens
# Compare
gutenberg_sherlock[21] # raw
sherlock_adventures[6] # clean
# We could also find these using a regular expression (regex)
# Convert the text to lowercase
sherlock_adventures <- tolower(sherlock_adventures)
gutenberg_sherlock <- tolower(gutenberg_sherlock)
# Find the part of the file to extract (end of target title to beginning of following title)
target_title <- sherlock_adventures[11] # The Adventure of the Missing Three-Quarter is the 11th element in the table of contents we scraped
following_title <- sherlock_adventures[12] # The title of the next story will help us know where to stop
# Now let's find the boundaries of the text span we want to extract
# *Part 1* Get the start of the span (end of the title)
# Detect the target title in the text
hits <- str_detect(gutenberg_sherlock, target_title)
# Which "hit" do we want to extract?
gutenberg_sherlock[hits] # 2 instances, 1 in table of contents, 2 in the text
# Use str_locate_all to locate every match we detected
hits <- as.data.frame(cbind(1:length(hits),
hits,
str_locate_all(gutenberg_sherlock,
target_title)))
colnames(hits) <- c("line", "detected", "location")
start <- hits %>% filter(detected == TRUE) %>%
select(line) %>% # select the line number col
last() %>% # to access the line numbers inside the col
nth(2) # second element from gutenberg_sherlock[detected]
# *Part 2* Get the end of the span (following title)
hits <- str_detect(gutenberg_sherlock, following_title)
# Which "hit" do we want to extract?
gutenberg_sherlock[hits] # 3 instances, 1 in table of contents, 2 in the text, 3 is something else
# Wrap the hit and span info in a dataframe again as we locate the hits
hits <- as.data.frame(cbind(1:length(hits),
hits,
str_locate_all(gutenberg_sherlock,
following_title)))
colnames(hits) <- c("line", "detected", "location")
end <- hits %>% filter(detected == TRUE) %>%
select(line) %>% # select the line number col
last() %>% # to access the line numbers inside the col
nth(2) # second element from gutenberg_sherlock[detected]
# Extraction
sherlock_three_quarter <- gutenberg_sherlock[start + 1 : end - 1]
head(sherlock_three_quarter) # Peek
# What additional processing do you think we need?
# Timeline of geopolitical changes (1900âˆ’present)
wiki_url <- "https://en.wikipedia.org/wiki/Timeline_of_geopolitical_changes_(1900%E2%88%92present)"
wiki_html <- read_html(wiki_url)
# Extract the tables from the HTML and combine them as a dataframe
wiki_tables <- wiki_html %>%  # read the html
html_nodes("#mw-content-text > div.mw-parser-output > table") %>%  # css selector rule
html_table() %>%  # convert HTML table to a df
bind_rows(.id = "tbl_order") # combine all the dfs, add new col
# Check result
wiki_tables %>% sample_n(10)
# Simplify the dataset
wiki_tables <- wiki_tables %>%
select(c("Year", "Event")) %>% # Keep only these columns
na.omit() # drop the leading blank row
# Check result
wiki_tables %>% sample_n(10)
get_sentiments("bing")
# install.packages("stringi")
# install.packages("stringr")
# install.packages("tokenizers")
# install.packages("gutenbergr")
# install.packages("tidytext")
# install.packages("rvest")
install.packages("topicmodels")
library(topicmodels)
head(sherlock_three_quarter) # Peek
sherlock_three_quarter_cln <- sherlock_three_quarter[3:length(sherlock_three_quarter)]
head(sherlock_three_quarter_cln) # Peek
# Make a cleaner version
sherlock_three_quarter_cln <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>%
paste(sherlock_three_quarter, collapse = " ")
head(sherlock_three_quarter_cln) # Peek
# Make a cleaner version
sherlock_three_quarter_cln <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>%
trimws() %>%
paste(sherlock_three_quarter, collapse = " ")
head(sherlock_three_quarter_cln) # Peek
# Make a cleaner version
sherlock_three_quarter_cln <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>%
trimws() %>%
head()
# Make a cleaner version
sherlock_three_quarter[3:length(sherlock_three_quarter)] %>%
trimws() %>%
head()
# Make a cleaner version
sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>%
head()
na.omit() %>%
head()
# Make a cleaner version
sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
unnest_lines() %>%
na.omit() %>%
head()
# Make a cleaner version
sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
unnest_tokens() %>%
na.omit() %>%
head()
# Make a cleaner version
sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
paste0(collapse = "") %>%
head()
# Make a cleaner version
sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>%
paste0(collapse = " ") %>%
head()
# Make a cleaner version
sherlock_three_quarter_clean <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>% # trim empty lines
paste0(collapse = " ") # Collapse the list of strings into one character vector
# Make a cleaner version
sherlock_three_quarter_clean <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>% # trim empty lines
paste0(collapse = " ") %>% # Collapse the list of strings into one character vector
unnest_tokens(word)
# Make a cleaner version
sherlock_three_quarter_clean <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>% # trim empty lines
paste0(collapse = " ") %>% # Collapse the list of strings into one character vector
as.data.frame() %>%
unnest_tokens(format = word)
# Make a cleaner version
sherlock_three_quarter_clean <- sherlock_three_quarter[3:length(sherlock_three_quarter)] %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>% # trim empty lines
paste0(collapse = " ") %>% # Collapse the list of strings into one character vector
as.data.frame() %>%
unnest_tokens(format = "text")
# Make a cleaner version
sherlock_clean <- tibble(txt = sherlock_raw[3:length(sherlock_three_quarter)])
# Extraction
sherlock_raw <- gutenberg_sherlock[start + 1 : end - 1]
# Make a cleaner version
sherlock_clean <- tibble(txt = sherlock_raw[3:length(sherlock_raw)])
# Make a cleaner version
sherlock_clean <- tibble(txt = sherlock_raw[3:length(sherlock_raw)]) %>% # trim title
trimws() %>%  # trim whitespace
na.omit() %>% # trim empty lines
#paste0(collapse = " ") %>% # Collapse the list of strings into one character vector
unnest_tokens(format = "text")
# Make a cleaner version
sherlock_clean <- tibble(txt = sherlock_raw[3:length(sherlock_raw)]) %>% # trim title
# trimws() %>%  # trim whitespace
# na.omit() %>% # trim empty lines
#paste0(collapse = " ") %>% # Collapse the list of strings into one character vector
unnest_tokens(word, txt)
head(sherlock_clean)
wiki_tokens <- wiki_tables %>%
group_by(Year) %>%
paste0(collapse = " ")
wiki_tokens <- wiki_tables %>%
group_by(Year) %>%
mutate(Event_By_Year = paste0(Event, collapse = ""))
View(wiki_tokens)
wiki_tokens <- wiki_tables %>%
group_by(Year) %>%
summarize(Event_By_Year = paste0(Event, collapse = ""))
wiki_tokens <- wiki_tables %>%
group_by(Year) %>%
summarize(Event_By_Year = paste0(Event, collapse = ""))  %>% # one row per year
unnest_tokens(word, Event_By_Year)
View(wiki_tokens)
View(wiki_tables)
# Check result
wiki_tokens %>% sample_n(10)
source("~/.active-rstudio-document", echo=TRUE)
# install.packages("stringi")
# install.packages("stringr")
# install.packages("tokenizers")
# install.packages("gutenbergr")
# install.packages("tidytext")
# install.packages("rvest")
# install.packages("topicmodels")
# install.packages("quanteda)
install.packages("quanteda")
library(quanteda)
wiki_tokens %>%
group_by(Year) %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
wiki_tokens %>%
group_by(Year) %>%
inner_join(nrc) %>%
count(word, sort = TRUE)
wiki_tokens %>%
group_by(Year) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sort = TRUE)
library(textdate)
library(textdata)
install.packages("textdata")
library(textdata)
wiki_tokens %>%
group_by(Year) %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sort = TRUE)
x <- get_sentiments("nrc")
View(x)
unique(x$sentiment)
sentiments <- get_sentiments("nrc")
unique(sentiments$sentiments)
unique(sentiments$sentiment)
sentiments <- get_sentiments("afinn")
wiki_tokens %>%
group_by(Year) %>%
left_join(sentiments)
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments)
View(wiki_sentiment)
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments) %>%
summarise(value)
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments) %>%
summarise()
wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments
)
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments) %>%
summarise(word)
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments) %>%
summarise(Year)
?summarise
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments) %>%
summarise(sentiment = mean(value))
wiki_sentiment <- wiki_tokens %>%
group_by(Year) %>%
inner_join(sentiments) # %>% summarise(sentiment = mean(value))
?saveRDS
saveRDS(sherlock_tokens, "part-1-sherlock.rds")
setwd("~/Git/r-txt-analysis-2021")
# Save our cleaned data
saveRDS(sherlock_tokens, "part-1-sherlock.rds")
saveRDS(wiki_tokens, "part-1-wiki.rds")
# You'll load these files in Part 3.
saveRDS(sherlock_tokens, "part-1-sherlock.rds")
saveRDS(wiki_tokens, "part-1-wiki.rds")
