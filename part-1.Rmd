---
title: "part-1"
author: "Dan Turner"
date: "7/28/2021"
output: html_document
---

# Part 1: Cleanining, extracting, tokenization

## Install packages if needed
```{r inistallatiion, message=FALSE, warning=FALSE}
# install.packages("stringr")
# install.packages("gutenbergr")
# install.packages("tidytext")
# install.packages("rvest")
# install.packages("topicmodels")
# install.packages("text2vec")
# install.packages("textdata")
# install.packages("stm")
# install.packages("quanteda")
# install.packages("quanteda.textplots")
# install.packages("quanteda.textstats")
# install.packages("purrr")
# install.packages("ggplot2")
```

## Setup R for Part 1

```{r}
# Load packages for part 1
library(dplyr) # modern syntax
library(gutenbergr) # easy access to gutenberg digital library
library(stringr) # most common string processing functions (uses stringi)
library(tidytext) # most common text processing functions
library(rvest)  # web scraping
library(textdata)
library(ggplot2)


# Load packages for part 3
# library(topicmodels)  # topic modeling
# library(quanteda) # new and powerful text analysis package for R
# library(text2vec)  # map ngram models in vector space to train CNNs
# library(quanteda.textstats)
# library(quanteda.textplots)

```

# Wrangling strings in R

We begin by downloading a book from the internet to supply some text to work with.

The first few steps of a text analysis project is usually the most time consuming. Once the data are in a certain format, there are usually premade modeling solutions that you can use.

The advantage of using R versus other languages is that the syntax for R is very clean and readable. This makes working with strings rather straightforward, especially using modern syntax. I'll be using tidy code, but not the brand-new R pipe (next version!).

## Cleaning

'Clean' is relative and you should never assume you've corrected every mistake. Text data is generally difficult to work with, and working with English brings its own issues. Any differences in syntax, spelling, affixing of words, irregular forms, geographical varieties, idiosyncratic capitalization or punctuation, etc. will be a confound for text analysis. Keep in mind that some errors are 'digital original' because of data entry and/or Optical Character Recognition mistakes.

In the next code chunk, we fetch a digital book to see how clean or dirty it is.


```{r}

# Find the gutenberg_id of a book to scrape

# Return only books where 'Doyle' is detected in the 'author' position
gutenberg_ids <- gutenberg_works(str_detect(author, "Doyle")) # read about str_detect() below

# Run this
gutenberg_sherlock <- gutenberg_download(108)$text # run this and check the result

# Download the raw text of your choice
gutenberg_text <- gutenberg_download(1)$text

# Here are some ids of popular books:
# 1342 = Pride and Prejudice by Jane Austen
# 11 = Alice's Adventures in Wonderland by Lewis Carroll
# 64317 = The Great Gatsby by F. Scott Fitzgerald

```

## stringr package
str_detect() is a stringr function that returns a logical TRUE if a pattern is detected in a string passed to it. Here, the string is the 'author' field in the object returned by gutenberg_works(), and the pattern is "Doyle". We could make a much better rule by writing a regular expression here instead, but we'll get to that!

You should know that the `stringr` package is a collection of very common text wrangling functions. It's built on a more powerful package, `stringi`, which has five times more functionality, but it's more difficult to use. If you ever need a very specific function and `stringr` is not working, check `stringi`.

## Checking the source

How clean is the data source? Do you consider the text before the first sentence of a book as part of the book, or do you want to restrict yourself to paragraphs of text?

The first step is seeing how the data is formatted, and measuring that against your goals. How much work does it need?

```{r sherlock}

# For The Return of Sherlock Holmes, the text begins on line 33
head(gutenberg_download(108)$text, 40)

```

## Scraping the Table of Contents

I like that the Table of Contents for this book is a list of adventures (recall how indexing works in R--this would be represented as [16:28]).

Let's scrape this list from the book's text, then extract our story of choice.

```{r scrape ToC}

# Import ToC as a cleani tibble
sherlock_adventures <- gutenberg_sherlock[16:28] %>% # range of lines that's the ToC
  trimws() %>% # trim extra whitespace around each line 
  str_replace_all("[\\p{P}\\p{S}&&[^-]]", "")# replace all punctuation (p) with a space (second argument) except (Subtract) initial hyphens using a Regular Expression -- see more below

# Compare
gutenberg_sherlock[21] # raw
sherlock_adventures[6] # clean

# We could also find these using a regular expression (regex) -- more on that later
```
## Regular expressions (REGEX)

Regex is a syntax for writing generalized patterns, usually among character vectors like text and numbers. If you want to identify, count, extract, or parse anything using pattern recognition, it's the typical starting place.

You will see many examples of it in this walkthrough, because it's one of the basic tools of text wrangling in R. I think it's difficult to read, but there's no alternative.

```{r regex-phone-num}

phones = "If you want to call the White House use 202-456-1111. But if you want to call Congress, use (202)224-3121."

# Example from https://cran.r-project.org/web/packages/stringr/vignettes/regular-expressions.html because it taught me how to comment within regex rules:
extract_phones <- regex("
  \\(?     # optional opening parens
  (\\d{3}) # area code
  [)- ]?   # optional closing parens, dash, or space
  (\\d{3}) # another three numbers
  [ -]?    # optional space or dash
  (\\d{3}) # three more numbers
  ", comments = TRUE)

str_match(phones, extract_phones)  # retreive the first match

# How do you get them all? See next code chunk.

```
## Regex examples

Because this is the trickiest part of working with text, let's look at some more handy functions for string wrangling.

```{r regex-2}

# Common stringr functions

str_match_all(phones, extract_phones)  # retrieve all matches; str_match() returns first match only

# str_detect(string, pattern) returns TRUE if the pattern is detected in the string
guides <- gutenberg_works(str_detect(title, "^Guide")) # if it's TRUE, it gets returned

str_count(guides, "Yosemite") # works like str_detect() but counts the TRUEs

str_match_all(guides, "Yosemite") # see the matches

str_replace_all(phones, "202", "313") # substitute x for y

str_split(guides$title, "[[:punct:]]|[[:digit:]]") # split string where rule detected

str_split_fixed(guides$title[1], " ", n = 6) # will let you apply the split on every nth detection/location

str_locate(phones, "White House") # what are the indexes?

# That means we can find the boundaries of matches like this:
str_locate(phones, "White House")[1] # W
str_locate(phones, "White House")[2] # e

```

## More regex tips
Note the useful [[:punct:]] and [[:digit:]] list codes. You can find more of them here: https://www.petefreitag.com/cheatsheets/regex/character-classes/

There are numerous regex 'generators' online, but I have only had mixed success with using these in real contexts. Usually it takes experimentation to get the correct result.


# String extraction

Often it is useful to split a text into smaller pieces. Either because that reflects its original state, or because it facilitates analysis. If you can't use str_split() functions to separate the parts, you will likely have to search for a more complex pattern. 

Extracting substrings is a matter of locating the boundaries. Let's find the beginning and end of one of the Sherlock stories, The Adventure of the Missing Three-Quarter.

For its beginning, we will take the end of the chapter title (as it appeared on paper) and extract everything up to the beginning of the following chapter's title.

```{r missing-three-quarter}

# Convert the text to lowercase
sherlock_adventures <- tolower(sherlock_adventures)
gutenberg_sherlock <- tolower(gutenberg_sherlock)

# Find the part of the file to extract (end of target title to beginning of following title)
target_title <- sherlock_adventures[11] # The Adventure of the Missing Three-Quarter is the 11th element in the table of contents we scraped

following_title <- sherlock_adventures[12] # The title of the next story will help us know where to stop

# Now let's find the boundaries of the text span we want to extract

# *Step 1* Get the start of the span (end of the title)

# Detect the target title in the text
hits <- str_detect(gutenberg_sherlock, target_title)

# Which "hit" do we want to extract?
gutenberg_sherlock[hits] # 2 instances, 1 in table of contents, 2 in the text

# Use str_locate_all to locate every match we detected
hits <- as.data.frame(cbind(1:length(hits),
                               hits,
                               str_locate_all(gutenberg_sherlock,
                                              target_title))) 

colnames(hits) <- c("line", "detected", "location")

start <- hits %>% filter(detected == TRUE) %>%
  select(line) %>% # select the line number col
  last() %>% # to access the line numbers inside the col
  nth(2) # second element from gutenberg_sherlock[detected]

# *Step 2* Get the end of the span (following title)
hits <- str_detect(gutenberg_sherlock, following_title)

# Which "hit" do we want to extract?
gutenberg_sherlock[hits] # 3 instances, 1 in table of contents, 2 in the text, 3 is something else

# Wrap the hit and span info in a dataframe again as we locate the hits
hits <- as.data.frame(cbind(1:length(hits),
                               hits,
                               str_locate_all(gutenberg_sherlock,
                                              following_title))) 

colnames(hits) <- c("line", "detected", "location")

end <- hits %>% filter(detected == TRUE) %>%
  select(line) %>% # select the line number col
  last() %>% # to access the line numbers inside the col
  nth(2) # second element from gutenberg_sherlock[detected]

# Extraction
sherlock_raw <- gutenberg_sherlock[start + 1 : end - 1]

head(sherlock_raw) # Peek

# What additional processing do you think we need?

# Make a tokenized version for analysis later (as a tibble, too use tidytext)
sherlock_tokens <- tibble(txt = sherlock_raw[3:length(sherlock_raw)]) %>% # trim title
  unnest_tokens(word, txt) # tidytext tokenization by word (space)

head(sherlock_tokens)

```


# HTML Tables: Scrape & clean a Wikipedia page table

Text data varies widely in how it's structured. For example, HTML tables online are in text format, but are easily scraped into a dataframe.

In the next code chunk, we scrape a wikipedia table (actually many tables) and we split it into individual words, a process called *tokenization*. 


```{r scrape-wikipedia-table}

# Timeline of geopolitical changes (1900−present)
wiki_url <- "https://en.wikipedia.org/wiki/Timeline_of_geopolitical_changes_(1900%E2%88%92present)"

wiki_html <- read_html(wiki_url)

# Extract the tables from the HTML and combine them as a dataframe
wiki_tables <- wiki_html %>%  # read the html
 html_nodes("#mw-content-text > div.mw-parser-output > table") %>%  # css selector rule
 html_table() %>%  # convert HTML table to a df
 bind_rows(.id = "tbl_order") # combine all the dfs, add new col

# Check result
wiki_tables %>% sample_n(10)

# Simplify the dataset
wiki_tables <- wiki_tables %>%
  select(c("Year", "Event")) %>% # Keep only these columns
  na.omit() # drop the leading blank row

# Check result
wiki_tables %>% sample_n(10)

wiki_tokens <- wiki_tables %>%
  group_by(Year) %>%
  summarize(Event_By_Year = paste0(Event, collapse = ""))  %>% # one row per year
  unnest_tokens(word, Event_By_Year) # tokenize while retaining year

# Check result
wiki_tokens %>% head(10)
```
## Are we done?
A quick inspection will show that this data has some errors in it. Are they the kind of error that can change our analysis? In this case no, but it's typical to go through many cleaning steps.

# Quick sentiment analysis

A sentiment analysis will tell us about the meaning of the text, in terms of its emotional content. It can answer whether Tweets are trending optimistic or negative, or what internet commentators think about a certain product. We will analyze the timeline from Wikipedia as an example.

```{r sentiment, message=TRUE, warning=FALSE}

# Approve download of this dataset if given the option
sentiments <- get_sentiments("afinn")

# Join the rated sentiments with the tokens and summarize by year
wiki_sentiment <- wiki_tokens %>%
  group_by(Year) %>%  # we want one row per year
  inner_join(sentiments) %>%  # keeps only tokens from wiki that have sentiment values
  summarise(avg_sentiment = mean(value)) # new column with mean of 'value' column

# Quick plot of our results
sentiment_plot <- ggplot(wiki_sentiment, aes(Year, avg_sentiment)) +
  geom_point() +
  geom_smooth(method = "lm")

sentiment_plot # view

```

# End of Part 1

We have acquired some different kinds of text from books and Wikipedia, cleaned them using string wrangling functions, tokenization, did some data extraction before a simple sentiment analysis.

## Here's what's coming up:
In Part 2 you'll have the option of practicing the skills I've introduced.
In Part 3 we take the texts we processed and apply some popular text analysis methods to them.


## Save progress for Part 3
```{r do not run}

# You'll load these files in Part 3.
# saveRDS(sherlock_raw, "part-1-sherlock-raw.rds")
# saveRDS(sherlock_tokens, "part-1-sherlock.rds")
# saveRDS(wiki_tokens, "part-1-wiki.rds")

```

